{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d64f35e5",
   "metadata": {},
   "source": [
    "\n",
    "# EDA-Only Cleaning Pipeline (No Train/Test, No NPZ)\n",
    "Use this notebook to **clean and standardize** your raw tables for exploratory data analysis (EDA).  \n",
    "It **does not** split train/test, encode features, or create NPZ files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612b32c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === CONFIG: set your file names and key columns here ===\n",
    "# Put the three CSVs in the SAME folder as this notebook.\n",
    "FILENAMES = {\n",
    "    \"main\": \"primary_table.csv\",          # <- change to your main fact table\n",
    "    \"dim1\": \"dim_customers.csv\",          # <- change to your first dimension table\n",
    "    \"dim2\": \"dim_regions.csv\",            # <- change to your second dimension table\n",
    "}\n",
    "\n",
    "# Keys used to join tables (left joins from main)\n",
    "JOIN_KEYS = {\n",
    "    \"dim1_on\": [\"customer_id\"],           # keys shared by main and dim1\n",
    "    \"dim2_on\": [\"region_id\"],             # keys shared by main and dim2\n",
    "}\n",
    "\n",
    "# Date columns present in the merged dataset (set only those you actually have)\n",
    "DATE_COLS = [\"dob\", \"open_date\", \"last_activity\"]\n",
    "\n",
    "# Columns that should be non-negative (clip at 0 if negative shows up)\n",
    "NON_NEGATIVE_COLS = [\"price\", \"amount\", \"quantity\", \"revenue\", \"num_vintage\"]\n",
    "\n",
    "# Optional: columns that are clear identifiers (skip winsorizing on these)\n",
    "ID_LIKE_COLS = [\"customer_id\", \"region_id\", \"order_id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8d065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def normalize_col(c: str) -> str:\n",
    "    if not isinstance(c, str): \n",
    "        return c\n",
    "    return c.strip().lower().replace(\" \", \"_\")\n",
    "\n",
    "def winsorize_series(s: pd.Series, low=0.01, high=0.99):\n",
    "    if s.isna().all():\n",
    "        return s\n",
    "    lo, hi = s.quantile([low, high])\n",
    "    return s.clip(lower=lo, upper=hi)\n",
    "\n",
    "print(\"✔ Imports ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca393a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === LOAD ===\n",
    "base = Path(\".\")\n",
    "paths = {k: base / v for k, v in FILENAMES.items()}\n",
    "for k, p in paths.items():\n",
    "    if not p.exists():\n",
    "        print(f\"⚠ File not found for '{k}': {p}. Update FILENAMES above.\")\n",
    "        \n",
    "df_main = pd.read_csv(paths[\"main\"])\n",
    "d1 = pd.read_csv(paths[\"dim1\"])\n",
    "d2 = pd.read_csv(paths[\"dim2\"])\n",
    "\n",
    "print(\"Shapes:\", { \"main\": df_main.shape, \"dim1\": d1.shape, \"dim2\": d2.shape })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === STANDARDIZE COLUMN NAMES ===\n",
    "for d in (df_main, d1, d2):\n",
    "    d.columns = [normalize_col(c) for c in d.columns]\n",
    "\n",
    "print(\"Sample of standardized columns in main:\", df_main.columns.tolist()[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ede6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === MERGE (Left joins from main) ===\n",
    "df = df_main.merge(d1, on=JOIN_KEYS[\"dim1_on\"], how=\"left\")\n",
    "df = df.merge(d2, on=JOIN_KEYS[\"dim2_on\"], how=\"left\")\n",
    "print(\"Merged shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79350f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === TYPE FIXES ===\n",
    "# Parse dates\n",
    "for c in DATE_COLS:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
    "\n",
    "# Convert object columns that look numeric into numeric\n",
    "obj_cols = df.select_dtypes(\"object\").columns.tolist()\n",
    "for c in obj_cols:\n",
    "    # attempt numeric coercion; if most values become NaN, revert to original\n",
    "    coerced = pd.to_numeric(df[c].str.replace(\",\",\"\", regex=False), errors=\"coerce\")\n",
    "    if coerced.notna().mean() >= 0.6:  # at least 60% convertible\n",
    "        df[c] = coerced\n",
    "\n",
    "print(\"Dtypes after coercion:\")\n",
    "print(df.dtypes.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e902b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === DERIVED FIELDS (optional) ===\n",
    "today = pd.Timestamp.today().normalize()\n",
    "\n",
    "if \"dob\" in df.columns:\n",
    "    age_years = ((today - df[\"dob\"]).dt.days / 365.25).round()\n",
    "    df[\"age\"] = age_years.clip(lower=0, upper=120)\n",
    "    # store as Int64 (nullable)\n",
    "    df[\"age\"] = df[\"age\"].astype(\"Int64\")\n",
    "\n",
    "if \"open_date\" in df.columns:\n",
    "    tenure_months = ((today - df[\"open_date\"]).dt.days / 30.44).round(0)\n",
    "    df[\"num_vintage\"] = tenure_months.clip(lower=0)\n",
    "    df[\"num_vintage\"] = df[\"num_vintage\"].astype(\"Int64\")\n",
    "\n",
    "print(\"Derived columns present:\", [c for c in [\"age\",\"num_vintage\"] if c in df.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === MISSING VALUES ===\n",
    "# Numeric: fill with median (overall)\n",
    "num_cols = df.select_dtypes(include=[np.number, \"Float64\", \"Int64\"]).columns.tolist()\n",
    "for c in num_cols:\n",
    "    med = df[c].median(skipna=True)\n",
    "    df[c] = df[c].fillna(med)\n",
    "\n",
    "# Categorical: fill with \"Unknown\"\n",
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].astype(\"string\").str.strip()\n",
    "    df[c] = df[c].fillna(\"Unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ee655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === INVALIDS & OUTLIERS ===\n",
    "# Clip common non-negative fields\n",
    "for c in NON_NEGATIVE_COLS:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].clip(lower=0)\n",
    "\n",
    "# Light winsorization for numeric columns (skip obvious IDs)\n",
    "skip = set(ID_LIKE_COLS)\n",
    "for c in num_cols:\n",
    "    if c not in skip and df[c].nunique(dropna=True) > 5:\n",
    "        df[c] = winsorize_series(df[c])\n",
    "\n",
    "# Replace any remaining inf\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "for c in num_cols:\n",
    "    med = df[c].median(skipna=True)\n",
    "    df[c] = df[c].fillna(med)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ba341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === DEDUP & USELESS COLUMNS ===\n",
    "before = df.shape[0]\n",
    "df.drop_duplicates(inplace=True)\n",
    "after = df.shape[0]\n",
    "print(f\"Removed {before - after} duplicate rows.\")\n",
    "\n",
    "# Drop near-constant columns or those >95% missing (post-fill check is trivial; we check before-fill via an aux copy)\n",
    "# (Here we approximate by checking variance / nunique)\n",
    "to_drop = []\n",
    "for c in df.columns:\n",
    "    if df[c].nunique(dropna=True) <= 1:\n",
    "        to_drop.append(c)\n",
    "\n",
    "if to_drop:\n",
    "    df.drop(columns=to_drop, inplace=True)\n",
    "    print(\"Dropped near-constant columns:\", to_drop)\n",
    "else:\n",
    "    print(\"No near-constant columns dropped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561027b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === SAVE CLEAN DATASET ===\n",
    "out_path = Path(\"clean_dataset.csv\")\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"✅ Saved: {out_path.resolve()}\")\n",
    "print(\"Preview:\")\n",
    "display(df.head(5))\n",
    "print(\"Shape:\", df.shape)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
